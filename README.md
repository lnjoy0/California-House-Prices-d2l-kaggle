# California-House-Prices-d2l-kaggle

沐神课程中的第一个比赛，kaggle地址：[California House Prices | Kaggle](https://www.kaggle.com/competitions/california-house-prices)

自己实现的MLP方法以及沐神讲的autogluon方法简介如下。

## 数据预处理

1. 发现bedrooms特征中有些是字符串，有些是数字。写一个函数处理该特征，将数字转为int类型，将字符串转为NaN。

2. 删除训练集中缺失值太多或不好填充的样本。

3. 删除无关列并且填充训练集和测试集中的缺失值。
   
   - 数值特征用均值填充；文本特征填充为"Unknow"；车库面积和上次销售价格的缺失值填充为0，因为没有车库面积等同于面积为0，没有上次售价说明是新房。
   
   - 添加一个新的二元特征，将新房（即上次售价为空的样本）标记为1。让模型更容易理解0这个异常值。

4. 处理时间特征。
   
   - 将原本的年月日改为距离2099-01-01的天数或年数。这样让模型更容易理解时间特征的含义（将时间特征改为具有大小含义的数值），即往往越小的值表示房子越新，价格可能越贵。

5. 调用大模型为Summary打分（效果似乎不好，后续注释掉了）
   
   1. 使用langchain和pydantic设置LLM格式化输出
   
   2. 使用langchain的ChatOpenAI初始化模型，这里使用deepseek api（因为比较便宜）
   
   3. 写系统提示词，指定模型角色、任务、打分标准（这里用gemini生成），以及输出格式（pydantic生成的）。
   
   4. 使用asyncio进行批量并发请求。因为deepseek-chat生成一个样本的输出大概要4秒左右，有7万多个样本要处理，所以一个一个跑得77个多小时才能跑完，而用批量大小为20的并发请求的话，只要3个多小时。（deepseek的生成速度相对较慢，但是好在它的api没有并发限制）
   
   5. （deepseek是相对较便宜的api了，不过跑完6万多个样本后仍然花了40多RMB）
   
   6. （这里思路是将LLM为Summary打的分数作为一个训练特征，从而处理Summary这个长文本特征。但是实验效果并不好，加上这个打分特征之后log rmse有0.7左右，而直接移除这个特征后log rmse降到了0.2左右。）

6. 数值特征标准化。
   
   - 这里将训练集和测试集拼接起来计算均值和标准差。

7. 价格相关特征以及标签的对数变换。
   
   - 例如挂牌价格、上次售价等价格特征，与预测值（售价）相关性较强，带有一定标签要素，直接标准化会损失其特征，但是保持原样的话又会与其他特征的尺度差距太大，导致模型不好训练。所以取其对数值。
   
   - 标签也取对数，这样使用均方损失训练时，损失的梯度不会太大，使得模型训练更稳定。

8. 处理类别特征。
   
   1. 清洗原始文本。删除噪音、统一格式和描述等。
   
   2. 语义合并。从原始token中归纳出若干个核心类别，并进行映射，从而缩小维度。
   
   3. 使用MultiLabelBinarizer进行多标签数据的独热编码
   
   4. （这里清洗文本和语义合并的具体代码都使用kimi生成，将每个特征的所有类别交给kimi让其生成处理代码）

9. 处理Zip码特征。
   
   - 原始的Zip特征是一个数值，模型很难从中提取出具体含义。而Zip码本身包含了大量位置信息，可以指示房屋所在的州、市、社区、街道等方位，很可能具有富人区和穷人区等经济意义。
   
   - 这里对于5位的Zip码，将每一位数字进行独热编码，共划分为5×10=50个特征。

## 模型训练

1. 读取训练数据和测试数据，预处理后转为张量。

2. 模型超参数设置如下：
   
   1. 采用五折交叉验证，训练周期数为300，学习率为0.005，权重衰减设置1e-3，批量大小为1024
   
   2. 动态学习率调度器`ReduceLROnPlateau`的`factor`为0.5，`patience`为15。即当连续15个周期的损失都没有显著下降时，自动将学习率设为原来的一半。从而避免“在山谷间来回乱跳“，进一步达到“谷底”。
   
   3. 设置三个dropout，分别为0.4、0.2、0.1。

3. 模型结构如下：
   
   1. 一个四层的MLP。各层输入输出维度分别为 (features_size, 1024)、(1024, 512)、(512, 256)、(256, 1)
   
   2. 各层之间加入了批量归一化、ReLU和Dropout。

4. 初始化权重和偏移。对所有线性层应用Kaiming初始化。

5. 训练时的损失函数使用均方损失，梯度下降使用Adam优化器，引入动态学习率调度器。

## 训练结果

采用以上预处理、超参数和训练方法，训练时的损失变化如下图。

![](https://cdn.nlark.com/yuque/0/2025/png/22329759/1766930298041-5f8192b1-3c7c-4bd9-9561-e979c68765db.png)

![](https://cdn.nlark.com/yuque/0/2025/png/22329759/1766930313682-8be597d9-6b37-4f22-a72b-0557b3c42a95.png)

提交Kaggle评分，结果如下。

![](https://cdn.nlark.com/yuque/0/2025/png/22329759/1766930416185-bfd50f58-c1cb-4996-b35f-b4ab8921198c.png)

## AutoML方案

使用AutoML（自动化机器学习）能以更少的代码量得到更好的效果，代价是训练时间较长。

以下是使用AutoGluon训练所得模型的Kaggle评分。

![](https://cdn.nlark.com/yuque/0/2025/png/22329759/1766932152406-95267f75-9461-4e20-a8b8-70e76ab7648c.png)
